WebApp Pentesting For Bug Hunting:
=================================
Basics:
	Aim:
		To spend months on a particular program
	General Advice for Bug Hunting:
		1. Avoid Dupes/Duplicates:
			Best way to avoide dupes is by avoiding low-hanging fruits if program age is more than 48 hours.
		2. Make sure to stay in Scope:
			Avoid scanning out of scope domains/subdomains.
			Avoid scanning out of scope subdirectories.
			Avoid testing & finding out of scope bugs
		3. Read Hacktivity:
			Basics:
				It allows to read all resolved reports on hackerone.
				Usage:
					It helps to answer questions such as:
						What types of bugs are the most popular?
						Which services are most attacked?
						How many bugs were resolved so far?
						How many researchers were revarded?
						When the last bugs were disclosed?
			Filtering:
				Reports can be sorted by popularity or age.
				Reports can be filtered by company or bug name or keywords.
Recon & Attack Methodology:(5 Steps)
	Basics:
	Step-1: Selecting a Target:(aka Program)
		Factors to consider:(4)
			A. Program launch date:
				more age => more dupes
			B. Program responsiveness:
				average time to resolve/fix a security issue
				more time => more chance of dupes
			C. The scope of the bug bounty program:
				bigger scopes: 
					low competition
					big companies have different teams, it means differnt jobs which means more number of mistakes being made
			D. Bug bounty rewards
	Step-2: Mapping Attack Surface:
		Step-2a: Doing the Recon:(5)
			Direct Recon:(2 Steps)
				Step-A: Finding Domains:(aka Root/TLC/Seeds)
					copy them from BB page
					if a bb program has line like "Everything owned by company is allowed":
						to find all of its assets
						Acquisitions:
							some sites are owned by company but' are not presnt on BB page, these sites are mostly not in use, but still online
							crunchbase.com
								lets say company A owns company X,Y,Z and has all scope BB program. company Z owns comapany P,Q,R. so, if i find a bug in company P, then, its a valid finding to report to company A
						ASN:(Autonomus System Number)
							Basics:
								An ASN is a unique number assigned to an AS(Autonomous System)(An Autonomous System is a set of routers, or IP ranges, under a single technical administration.)
								These ASN’s helps us to picture the entity’s IT infrastructure.
							Enumeration:
								Manually:
									bgp.he.net
										search for "company name"
										Issue:
											doesnt represent cloud spaces owned by company
								Automated Tools:
									metabigor:(github)
										uses bgp.he.net and asnlookup.com
										echo "comp_name" | metabigor --org -v
									asnlookup:(github)
										python asnlookup.py -o <comp_name>
									Amass:(best)
										amass intel -asn <number>
						Reverse Whois:
							Basics:
								It tells all domains registered to company.com
							Manual:
								whoxy.com
							Automated:
								Amass:
									amass intel -whois -d domain.com -o out.txt
									Note:
										Read Amass Section below to read more.
								Domlink:
									Basics:
									Usage:
										domlink.py -d site.com -o site.txt
										Flags:
											-d => 
											-o => 
						Ad Analytics and relationships:
							Basics:
								site.com and all its owned domains have same google analytics code and this way, we can find other domains attached with a site
							Manual:
								builtwith.com/relationships/
									search "site.com"
									goto "tag historry and relationships" => other domains
							Automated way:
								getrelationship.py:(https://github.com/m4ll0k/BBTz/blob/master/getrelationship.py)
									Basics:
									Usage:
										echo "site.com" | python3 getrelationship.py
				Step-B: Subdomains List creation:(2 Steps)
					Basics:
						Juicy Subdomains:
							*testsite
							*dev
							*corp
							*stage
							*dasboard
						Note:
							ww2.site.com is a genuine subdomain.
					Step-1: Collecting Live Subdomains:(into all_urls.txt)
						Methods:(2)
							1. Finding Live Subdomains from Live/Dead Subdomains:
								Basics:
									Methods explained below are used to find subdomains which may or may not be live.
									So, We need to use 'HTTPProbe' with them to filter out live subdomains.
								Step-1: Findng Live/Dead Subdomains:(3 Methods)
									1. Linked discovery:
										Basics:
											Linked Discovery is about finding subdomains using a Spidering tool such as BurpSuite.
										Manual:
											BurpSuite:(3 Steps)
												Step-1: Configuration:
													target -> scope -> enable "advanced Scope Control"
													target-> scope -> include in scope -> Add -> host: site_name -> ok
														site_name is our keyword here
													target -> sitemap -> filter -> enable "show only in-scope items"
												Step-2: Spidering:
													select all targets in sitemap -> right click -> spider these targets
														Note:
															do this recursively, until there is nothing left to find
													this will show us newly found subdomains and even new seed domains and it will higlight them
												Step-3: Output Manipulation:
													target -> sitemap -> select all targets -> right-click -> engagement tools -> analyze target -> save report as file.html file
													open file.html -> go to target section => copy all targets from there
												Note:
													Read WebApp Pentesting For CTF Sheet to read more.
										Automated:
											Gospider:(apt-get install gospider)
												Basics:
												Usage:
													gospider -q -s "https://google.com/"
													to get only subdomains:(https://github.com/harsh-kk/one_liner)
														gospider -d 0 -s "https://site.com" -c 5 -t 100 -d 5 --blacklist jpg,jpeg,gif,css,tif,tiff,png,ttf,woff,woff2,ico,pdf,svg,txt | grep -Eo '(http|https)://[^/"]+' | anew
													Flags:
														-q => Quiet mode, only print urls.
														-o output.txt
														-s "site"
														-S sites.txt
															Note:
																sites.txt should have http://site.com, not site.com
											Hakrawler:(github.com/hakluke/hakrawler)
												Basics:
												Usage:
													hakrawler -url bugcrowd.com -depth 1
													Flags:
														-url => 
														-depth => 
									2. JS Analysis:
										Basics:
											JS Analysis is about finding subdomains inside JS files.
											Note:
												Finding API keys or creds embedded in JS files is a valid vulnerability and is called 'Sensitive Data Exposure'.
										Automated:
											Subdomainizer:
												Basics:
													It reads JS files for both subdomains & for sensitive info such as private API keys/creds.
												Usage:
													Flags:
														-u <url> => 
														-l <list_of_urls> => 
														-o <out.txt> => 
											Subscrapper:
												Basics:
													It reads JS files for subdomains only.
												Usage:
													Flags:
									3. Subdomain Scrapping:
										Basics:
											Subdomain Scrapping is about finding subdomains by using search engines.
										Manual:
											Read Passive Information Gathering Sheet. 
										Automated:
											Amass:
												amass enum -df domains.txt -o out.txt
												Note:
													Read Amass Section below to read more.
											Shosubgo:
												Basics:
													Scraps subdomains using Shodan.io & requires it's API key to operate.
													Note:
														Amass can also be used for same by providing it with API key of shodan.io
												Usage:
													Flags:
								Step-2: Filtering out Live Subdomains:
									Httprobe:(apt-get install httprobe)
										Basics:
										Usage:
											echo domains.txt | httprobe > final_domains.txt
							2. Finding Live Subdomains directly:
								Basics:
									Subdomains found via 'Subdomain Bruteforcing' are LIVE/ONLINE, so no need to use httprobe with them.
								Methods:(1)
									1. Subdomain Bruteforcing:
										Basics:
										Automated:
											Read WebApp Pentesting For CTF Sheet.
					Step-2: Manipulating Url List:
						Step-2a: Unique Urls only
							cat all_urls.txt | uniq > final_urls.txt
						Step-2b: No 'https://' or 'http://' Suffix
							Remove 'https://' & 'http://' suffix(if present) from front of every subdomain present in 'final_urls.txt'.
			Indirect Recon:
				A. Github Dorking for Recon:(youtube.com/watch?v=l0YsEk_59fQ)
					Note:
						Read Github Sheet for basics.
					Basics:
					Manual Dorking:
						1. Using keywords:
							Keywords Collection:
								https://github.com/random-robbie/keywords/blob/master/keywords.txt
							Important Keywords:
								security_credentials => LDAP Creds
								connectionstring => DB Creds
								jdbc => OracleDB Creds
								ssh2_auth_password => Unauthorized access to servers
								send_keys => Password
							Examples:
								"company.com" ssh
								"company.com" config
							Note:
								Acting with intelligence is best bet in this case, for example, If a company uses apache server (we know via Wappalyzer/Builtwith), then we only need to look for sensitive files which belong to an apache server, not an IIS server.
							Note:
								If scope is big, then keywords like password, etc will give many many results, so focus on using keywords such as "jdbc"/"vsphere" instead.
						2. many search results dont belong to company or its developers, so no use of checking them
						3. sort by: "recently indexed" => better chances of finding unreported data exposure. Look for files as old as 1 week/month
						4. "NOT" Filter:
							lets say, we have a subdomain out-of-scope and it has a lot of search results, so to save time:
								"company.com" ssh NOT subdomain.company.com
							NOT removes every result which includes <string> in it.
						6. org:company_name => looks for official repo of that company.
							Note:
								we should not look in official repo of company/registered emloyees as its a waste of time.
						7. Always review Employee's other code/repo:
							Once, we find code from unregistered employee, visit his profile, look for him on linkedin, it will tell us if he is ex-employee or current employee.
							Once, we know he is a current employee:
								user:<username> password => to look if he sharerd his creds anywhere related to company or maybe he uses same password everywhere
						Note:
							If sensitive data related to internal server is found but a POC cant be created for same, then, submit written report without any POC to company and its upto them, whether to accept it or not.
					Automated Dorking:
						GitRob:(github.com/michenriksen/gitrob)
							Basics:
							Usage:
								Steps:
									Step-1: Configure gitrob
										gitrob --configure
									Step-2: Enter github user creds
										user:<my_github_username>
										password:<my_github_password>
									Step-3: Dork github
										gitrob -o <company_name>
									Step-4: See output:
										http://127.0.0.1:9393/ => Ouput reports
								Flags:
									--configure => 
									-o <> => 
						Note:
							Automated github dorking is not recommended as automated tools visit main page of an organization, search through their code/repo and then it goes to "people" tab (list of people who are "joined to that company on github").
							However, the issue is that, there maybe possibly many developers of company who are not connected to organization's github repo formally, thus, due to lack of this input, these tools dont search their profile, so most of possibly vulnerable scope is not even touched.
				B. Wayback Machine for Recon:
					Waybackurls:(github.com/tomnomnom/waybackurls)
						Basics:
						Usage:
							cat domains.txt | waybackurls > urls
							Flags:
				C. Shodan.io & CenSys.io:
					Shodan.io:
						Note:
							Shodan.io Recon is based on using filters to find "Hidden Assets" and to use FILTERS, we need to PAY.
						Common filters:(no use)
							os:"linux 2.6.x"
							product:"apache tomcat" version:"1.16.2"
							port:
						Useful Filters:
							org:"tesla motors"
							ssl:"target" => it looks in SSL for "target" string
							-"AkamaiGhost" => removes results having "AkamaiGhost" string in them
							html:"jenkins" => "html:" Searches full HTML content of the returned page and look for string(paid)		
						Choosing a subdomain to take first:
							first target those subdomains whose user interface is of common monitoring tools/CMS
							in case of custom webapp, target those subdomains whose user interface deviates from the common company’s theme.
								we get to know how a site looks using eyewitness tool
					CenSys.io:
		Step-2b: Finding already diclosed bugs in that program:
			Read hacktivity for that program
			google: site.com XSS vulnerability
	Step-3: Enumerating the selected Subdomains:
		Step-3A: Screenshot 'Subdomain.domain.com':
			Basics:
				Here, a screenshot of every subdomain in 'final_urls.txt' is taken. This helps to understand which subdomains have a login page as front page.
			Eyewitness:(apt-get install eyewitness)
				Basics:
					It looks for both http and https
				Usage:
					eyewitness.py -f final_domains.txt
					Flags:
						-t timeout_in_seconds
							timeout means time taken by tool to render the site and take a screenshot
							-t 15 => toll will only take a screenshot if time to render is less than 15 seconds
						-d directory
							where to save all the screenshots
							by default: inside eyewitness directory
		Step-3B: Port Scanning:
			Port scanning single Subdomain:
				Nmap:
					nmap -sC -sV <ip> -oA site_name
					Note:
						Read Active Information Gathering Sheet to read more.
			Port scanning mutiple Subdomains:
				Masscan:
					masscan -p1-65535 -iL ip_list --max-rate 18-- -oG out.log
					Note:
						Read Active Information Gathering Sheet to read more.
				DNSMasscan:(https://github.com/rastating/dnmasscan)
					dnsmasscan domains.txt dns.log -p1-65535 -oG masscan.log
					Note:
						Read Active Information Gathering Sheet to read more.
		Step-3C: Scanning for CVE's:
			Nuclei:(apt-get install nuclei)
				Basics:
					Nuclei is a configurable scanner based on templates that allows complete extensibility with a very simple templating syntax.
				Usage:
					nuclei -l urls.txt -t template.yaml
					Flags:
						-l => 
						-t => 
		Step-3D: SSL Testing:
			Basics:
			Nmap:
				nmap -p 443 --script=ssl-enum-ciphers site.com
				Note:
					Read Active Information Gathering Sheet to read more.
		Step-3E: Directory Bruteforcing:
			Read WebApp Pentesting For CTF.
		Step-3F: Static Code Review:
			Comment Hunting:
				Basics:
					This can help to find:
						A. Creds/Juicy Info
						B. Disabled Functionality:
							It can reveal past/future section of a webapp.
			Manual Hunting:
				Open source-code -> search "<!--"
	Step-4: Attacking WebApp Logic of Subdomains:
		Basics:
		Step-4A: Mapping features of webapp:
			Basics:
			Methodology:
				Step-4Aa: Functionality Access Testing:(aka Authenciation Flow Bypass)
					Basics:
						Use webapp as unauthenciated user & then Use webapp as authenciated user.
						Then try to access authenicated-only resources/functions as an unauthenciated user.
					Working:
						Make an account on site and visit all urls, then logout.
						Visit every url from BurpSuite History as an unauthenciated user.
						If we are able to access an authenticated-user only URL/Resource, then its a Finding.
				Step-4Ab: For an E-Commerce WebApp:
					1. Testing Customer Side:
						Create an order using a fake credit card & see if its getting billed.
					2. Testing Vendor/Seller Side:
						Vendor side has better chances of having a bug as front facing side of a webapp is mostly customer side, not vendor side.
				Step-4Ac: List all Juicy Functionalities:
					Look out for webapp features and make notes of interesting ones.
					For Example:
						File/Image/Pdf/Video Uploads
						Data Export/Import
						Text/Markdown Editors
				Note:
					Make sure to capture all the traffic with Burp during this step.
		Step-4B: Testing Injection at Get/Post parameters:
			Read SQL Injection Sheet.
		Step-4C: Testing Low-Hanging Fruits:
			Note:
				This section is only applicable for 'Bug Hunting' if the selected bug bounty program age is less than 2 days as most(80%) low hanging fruits are caught quickly due to shit-load amount of bug bounty hunters.
			Basics:
				Defining what bug is a low hanging fruit is a challenge on its own, Therefore, it can depend on researcher to researcher about what bug can be considered as a low-hanging fruit. 
				According to me, however, the following bugs are not Low-Hanging:
					1. SSRF
					2. CSRF
					3. SSTI
			Step-4Ca: Testing for XSS:
				Read Cross-Site Scripting Sheet.
			Step-4Cb: Testing for Open-Redirect:
				Read Open-Redirect Sheet.
			Step-4Cc: Testing for LFI:
				Read File Inclusion Sheet.
		Step-4D: Attacking Forums:
			Basics:
			Step-5A: Attacking Login Forums:
				Read WebApp Pentesting For CTF Sheet.
			Step-5B: Attacking Sign up Forums:
			Step-5C: Attacking Forgot Password Forums:
			Step-5D: Attacking Send Message/Chat/Reply Forums:
			Step-5E: Attacking CMS Forums:
				Read Attacking CMS Section below.
		Step-4E: Attacking CMS:
			Read WebApp Pentesting For CTF Sheet
		Step-4F: Attacking Upload Functionality:
		Step-4G: Attacking Authenciation & Defense Mechanism of WebApp:
			Go to Burp and look out for following questions:
				How authentication is made?
				Does the application use a third-party for that?
				Is there any OAuth flow?
				Is there any CSRF protection?
					If yes, how is it implemented?
				Are there any resources referenced using numerical identifiers?
					If yes, is there any protection against IDOR vulnerabilities?
				Does the application use any API?
				How does the application fetch data?
				Does it use a front-end Framework?
				What JavaScript files contain calls to the API?
				Does it use a back-end Framework?
					If yes, what is it and which version is being used?
		Step-4H: Testing other bugs:
			Read Vulnerabilities Section.
	Step-5: Attacking other ports of Subdomains:
		Read Active Information Gathering Sheet.
Amass:(apt-get install amass)
	Basics:
		Modes:(5)
			1. Intel:
				Discover targets for enumeration.
			2. Enum:
				Performs enumeration and network mapping.
			3. Viz:
				Visualize enumeration results.
			4. Track:
				Tracks difference between different enumeration reports.
			5. Db:
				Manipulates the Amass graph database.
		Config Files:
			~/.config/amass/config.ini:
				It stores all API keys.
	Usage:
		amass intel -whois -d domain.com -asn <asn_number>
		amass enum -d domain.com -asn <asn_number>
		amass enum -brute -d domain.com -w word.txt -asn <asn_number>
		amass enum -brute -df domains.txt -w wordlist.txt -rf resolvers.txt -o out.txt
		Flags:
			-d <domain> => 
			-df <domains.txt> 
				Note:
					domains.txt should have site.com, not https://site.com 
			-asn <asn_number> => 
			-o <out.txt> => 
			-p p1,p2 => 
			-ip => Show ip of assets found
			-config <> => Use specified config file
			-active => Amass sends traffic to assets to get TLS cert
			-pasive => Be passive
			-whois => Perform reverse whois
			-brute => Subdomain bruteforcing
			-w wordlist.txt => 
				Lists:
					https://gist.githubusercontent.com/jhaddix/86a06c5dc309d08580a018c66354a056/raw/96f4e51d96b2203f19f6381c8c545b278eaa0837/all.txt
			-nf names.txt => Do not find subdomains specified in names.txt
			-rf <resolvers.txt>
				Lists:
					https://raw.githubusercontent.com/janmasarik/resolvers/master/resolvers.txt
						Owner of above repo updates the resolvers.txt periodically
			Note:
				When we dont specify neither -passive nor -active flag, then, amass uses DNS request traffic to send to resolvers.
